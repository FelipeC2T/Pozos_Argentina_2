{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5143873",
   "metadata": {},
   "source": [
    "# ARGENTINE GOVERNMENT DATA (OIL AND GAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaffca",
   "metadata": {},
   "source": [
    "CSV data set used in this notebook downloable at: https://bit.ly/3UwPbbG \n",
    "\n",
    "Data property of:\n",
    "*Secretaría de Energía. Subsecretaría de Planeamiento Energético. Dirección Nacional de Escenarios y Evaluación de Proyectos. Dirección de Información Energética.Tecnología de la Información.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10559125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#Esta es la imagen satelital y el área de consesion (intentar con QGIS)\n",
    "#Image('img/DSC_0330.JPG', width=1200 , height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61103555",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "This is a very complete and large data set, so our first step consists of exploratory data analysis (EDA). Although data is provided at a monthly frequency, to reduce complexity we will work with data on a yearly frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9df85b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import geopandas as gpd\n",
    "import urllib\n",
    "import missingno as msno\n",
    "from termcolor import colored\n",
    "\n",
    "# Set stylistic options for plots generated later on\n",
    "sns.set_context(\"paper\")\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4def5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data\n",
    "data_url = 'http://datos.energia.gob.ar/dataset/c846e79c-026c-4040-897f-1ad3543b407c/resource/b5b58cdc-9e07-41f9-b392-fb9ec68b0725/download/produccin-de-pozos-de-gas-y-petrleo-no-convencional.csv'\n",
    "datafile = False\n",
    "\n",
    "\n",
    "# Define some colors for later on\n",
    "class color:\n",
    "    GREEN = 'green'\n",
    "    RED = 'red'\n",
    "    BOLD = 'bold'\n",
    "\n",
    "\n",
    "def check_and_load_dataset():\n",
    "    try:\n",
    "        df = pd.read_csv('arg_gov_oil_and_gas_dataset.csv')\n",
    "        print(colored('\\n===== DATA SET FOUND! =====', color.GREEN))\n",
    "        print('Reading into data frame...')\n",
    "        print(colored('Data read successfully!', color.GREEN))\n",
    "        return df, True\n",
    "    except FileNotFoundError:\n",
    "        print(colored('\\n====== DATA SET NOT FOUND OR UNREADABLE! ======', color.RED))\n",
    "        return None, False\n",
    "\n",
    "def download_dataset(data_url):\n",
    "    print('Downloading data set. This may take a while, please wait...')\n",
    "    try:\n",
    "        urllib.request.urlretrieve(data_url, 'arg_gov_oil_and_gas_dataset.csv')\n",
    "        print(colored('Data set downloaded successfully!', color.GREEN))\n",
    "        return pd.read_csv('arg_gov_oil_and_gas_dataset.csv'), True\n",
    "    except urllib.error.URLError:\n",
    "        print(color('\\n====== COULD NOT DOWNLOAD DATA SET! Check internet connection and/or file/directory permissions and try again. =====', Color.RED + Color.BOLD))\n",
    "        return None, False\n",
    "\n",
    "def user_prompt():\n",
    "    while True:\n",
    "        data_prompt = input('\\nDownload the dataset (approximately 100MB)? [yes]/no: ').lower()\n",
    "        if data_prompt in ['yes', 'y', '']:\n",
    "            return True\n",
    "        elif data_prompt == 'no':\n",
    "            print(colored('\\n====== DATA SET NOT DOWNLOADED! Please download the dataset manually and place it in the working directory before proceeding. =====', Color.RED + Color.BOLD))\n",
    "            return False\n",
    "        else:\n",
    "            print(colored('Invalid input. Try again.', color.RED))\n",
    "\n",
    "def main():\n",
    "    global data_url  # Declare data_url as a global variable\n",
    "    df, datafile = check_and_load_dataset()\n",
    "\n",
    "    if not datafile:\n",
    "        if user_prompt():\n",
    "            df, datafile = download_dataset(data_url)\n",
    "\n",
    "    # Rest of your main code here\n",
    "    if datafile:\n",
    "        # Continue with the rest of your analysis or processing\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            data_url = 'http://datos.energia.gob.ar/dataset/c846e79c-026c-4040-897f-1ad3543b407c/resource/b5b58cdc-9e07-41f9-b392-fb9ec68b0725/download/produccin-de-pozos-de-gas-y-petrleo-no-convencional.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cf7d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      "====== DATA SET NOT FOUND OR UNREADABLE! ======\u001b[0m\n",
      "\n",
      "Download the dataset (approximately 100MB)? [yes]/no: yes\n",
      "Downloading data set. This may take a while, please wait...\n",
      "\u001b[32mData set downloaded successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d667ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd855b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df, color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62ca36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idempresa: 41 distinct values\n",
      "anio: 18 distinct values\n",
      "mes: 12 distinct values\n",
      "idpozo: 3807 distinct values\n",
      "prod_pet: 89833 distinct values\n",
      "prod_gas: 136875 distinct values\n",
      "prod_agua: 84554 distinct values\n",
      "iny_agua: 226 distinct values\n",
      "iny_gas: 148 distinct values\n",
      "iny_co2: 1 distinct values\n",
      "iny_otro: 1 distinct values\n",
      "tef: 13891 distinct values\n",
      "vida_util: 467 distinct values\n",
      "tipoextraccion: 12 distinct values\n",
      "tipoestado: 17 distinct values\n",
      "tipopozo: 7 distinct values\n",
      "observaciones: 197 distinct values\n",
      "fechaingreso: 8519 distinct values\n",
      "rectificado: 2 distinct values\n",
      "habilitado: 1 distinct values\n",
      "idusuario: 41 distinct values\n",
      "empresa: 41 distinct values\n",
      "sigla: 3623 distinct values\n",
      "formprod: 26 distinct values\n",
      "profundidad: 2285 distinct values\n",
      "formacion: 26 distinct values\n",
      "idareapermisoconcesion: 107 distinct values\n",
      "areapermisoconcesion: 107 distinct values\n",
      "idareayacimiento: 146 distinct values\n",
      "areayacimiento: 145 distinct values\n",
      "cuenca: 4 distinct values\n",
      "provincia: 6 distinct values\n",
      "coordenadax: 3584 distinct values\n",
      "coordenaday: 3439 distinct values\n",
      "tipo_de_recurso: 1 distinct values\n",
      "proyecto: 2 distinct values\n",
      "clasificacion: 4 distinct values\n",
      "subclasificacion: 9 distinct values\n",
      "sub_tipo_recurso: 3 distinct values\n",
      "fecha_data: 216 distinct values\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    num_distinct_values = len(df[column].unique())\n",
    "    print(f\"{column}: {num_distinct_values} distinct values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the columns are and what shape our dataset is in\n",
    "print('Columns in the data frame.')\n",
    "print(df.columns,df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need all these columns. Let's take what we need for now, and leave what we don't behind (commented out)\n",
    "df=df[['idempresa', 'anio', 'mes', 'idpozo', 'prod_pet', 'prod_gas',\n",
    "       'prod_agua',\n",
    "      #'iny_agua', 'iny_gas', 'iny_co2', 'iny_otro', 'tef',\n",
    "       #'vida_util', 'tipoextraccion', \n",
    "      'tipoestado', 'tipopozo',\n",
    "      # 'observaciones', 'fechaingreso', 'rectificado', 'habilitado',\n",
    "      # 'idusuario',\n",
    "      'empresa', 'sigla', 'formprod', 'profundidad', 'formacion',\n",
    "       'idareapermisoconcesion', 'areapermisoconcesion', \n",
    "      'idareayacimiento', 'areayacimiento', 'cuenca', 'provincia', 'coordenadax', 'coordenaday',\n",
    "       'tipo_de_recurso', 'proyecto', 'clasificacion', 'subclasificacion',\n",
    "       'sub_tipo_recurso', 'fecha_data']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop an index if it doesn't have a valid anio/año (year) or mes (month) columns since that makes it are useless to us.\n",
    "df = df.dropna(subset=['anio', 'mes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data set has some problems with the anio column, let's make sure it's a valid int\n",
    "df['anio'] = pd.to_numeric(df['anio'], errors='coerce').astype(int)\n",
    "# Generate fecha (data) datetime-typed column out of anio and mes\n",
    "df['fecha'] = pd.to_datetime(df['anio'].astype(str) + '-' + df['mes'].astype(str))\n",
    "# Now let's see...\n",
    "print('Dates we generated, sorted from oldest to newest. They should be formated YYYY-MM-DD if all is right.\\n')\n",
    "print(df.sort_values(by=['fecha'])['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67591ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check total invalid entries for each column\n",
    "print('Sum of invalid entries per column. Looks like nothing that is of primary interest to us has invalid entries.\\n')\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_count():\n",
    "    return pd.DataFrame({'features': df.columns,\n",
    "                'dtypes': df.dtypes.values,\n",
    "                'NaN count': df.isnull().sum().values,\n",
    "                'NaN percentage': df.isnull().sum().values/df.shape[0]}).style.background_gradient(cmap='Set3',low=0.1,high=0.01)\n",
    "null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b82ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T.style.background_gradient(axis=0, cmap='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19728c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate axes ticks\n",
    "tick_spacing = 4\n",
    "ticks = [df['anio'].min() - 1]\n",
    "while np.max(ticks) < df['anio'].max():\n",
    "    ticks.append(np.max(ticks) + tick_spacing)\n",
    "\n",
    "# Generate the plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot( x = df['anio'], \n",
    "            color='#5092AB',\n",
    "            #linecolor = 'black',\n",
    "            flierprops={\"marker\": \"D\", \"markerfacecolor\" : \"#5092AB\"}\n",
    "           )\n",
    "ax.set_xlim(np.min(ticks) - 1, np.max(ticks) + 1)    # Need to +-1 to make sure ends of graph and ticks not cut off\n",
    "plt.xticks(ticks)\n",
    "plt.title('Range of years present in dataset')\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data with outlier years via inter-quartile range (IRQ)\n",
    "\n",
    "#Calculate IQR\n",
    "Q1 = df['anio'].quantile(0.25)\n",
    "Q3 = df['anio'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define upper and lower limits for identifying outliers\n",
    "lower_lim = Q1 - 1.5 * IQR\n",
    "upper_lim = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the outliers from the dataframe\n",
    "df['anio']=df['anio'][(df['anio'] >= lower_lim)]\n",
    "\n",
    "hist_df = df.groupby(['anio']).size().reset_index(name='count')\n",
    "hist_df['anio'] = (hist_df['anio'].astype(int)).astype(str)\n",
    "\n",
    "#print(hist_df)\n",
    "\n",
    "\n",
    "# Plot a histogram\n",
    "fig = sns.barplot(data = hist_df,\n",
    "                  x = 'anio',\n",
    "                  y = 'count',\n",
    "                  #hue = 'count',\n",
    "                  width = 1,\n",
    "                  palette = 'coolwarm',\n",
    "                  alpha = 0.8\n",
    "                 )\n",
    "\n",
    "\n",
    "sns.histplot(data=hist_df, x='anio', bins=10, kde=False, color='blue', alpha=0.8)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('# of data points')\n",
    "fig.set_xticklabels(hist_df['anio'], rotation=60)\n",
    "plt.title('Distribution of data points according to year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_EDA.csv',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36228c21",
   "metadata": {},
   "source": [
    "# EXTRA INFORMATION ABOUT DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167ed4f",
   "metadata": {},
   "source": [
    "## We can see that, thankfully, most data is recent.\n",
    "\n",
    "## We're gonna keep from the entries from the three companies with the most data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaeb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and count number of datapoints by company ID, then sort by count\n",
    "df_idempresa = df.groupby(['idempresa']).size().reset_index(name='datapoints').sort_values(by = ['datapoints'], ascending = False)\n",
    "\n",
    "# List the top three companies\n",
    "print('Top three companies in terms of datapoints')\n",
    "print(df_idempresa.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116da7f",
   "metadata": {},
   "source": [
    "## The three companies with the most datapoints are YPF, APS, and PLU. We'll proceed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb788f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "YPF = df.query('idempresa==\"YPF\" and 2010 <= anio')\n",
    "APS = df.query('idempresa==\"APS\" and 2010 <= anio')\n",
    "PLU = df.query('idempresa==\"PLU\" and 2010 <= anio')\n",
    "\n",
    "# Create a histogram \n",
    "fig = sns.barplot(data = df_idempresa,\n",
    "                  x = \"idempresa\", \n",
    "                  y = \"datapoints\",\n",
    "                  #hue = 'datapoints',\n",
    "                 # hue_norm = LogNorm(vmin=df_idempresa['datapoints'].min(), vmax=df_idempresa['datapoints'].max()),\n",
    "                  palette = 'coolwarm',\n",
    "                  alpha = 0.8,\n",
    "                  #legend = None,\n",
    "                  width = 1)\n",
    "fig.set_yscale(\"log\")\n",
    "plt.grid(which = 'both', axis = 'y')\n",
    "plt.title('Datapoints by company')\n",
    "plt.xlabel('Company ID')\n",
    "plt.ylabel('# of datapoints')\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265b5ea",
   "metadata": {},
   "source": [
    "## Note the above graph is *logarithmic* in scale. \n",
    "\n",
    "## Let's concatenate data from the top three companies into a new data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['YPF','APS','PLU']\n",
    "df_final=df.query('idempresa == @companies and 2010 <= anio <= 2023').sort_values(by = ['idpozo']).reset_index()\n",
    "\n",
    "#Let's see the \"final\" data frame after all our filtering\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a060ede",
   "metadata": {},
   "source": [
    "## We graph production by company by year. Again, note the logarithmic scale of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = df_final.groupby(['idempresa', 'fecha'])['prod_pet'].sum().reset_index()\n",
    "companies = df_prod['idempresa'].unique()\n",
    "years = np.sort(df_final['anio'].unique().astype(int))\n",
    "\n",
    "plt.subplots(figsize = (10,10))\n",
    "fig = sns.histplot(data = df_prod,\n",
    "                   stat = 'count',\n",
    "                   x = 'fecha',\n",
    "                   multiple = 'layer',\n",
    "                   hue = 'idempresa',\n",
    "                   weights = 'prod_pet',\n",
    "                   palette = 'coolwarm_r',\n",
    "                   alpha = 0.7,\n",
    "                   bins = len(years),\n",
    "                   legend = False\n",
    "                   )\n",
    "plt.legend(title='Company ID', loc='upper left', labels=['YPS', 'PLU', 'APS'])     # Can double check correct via legend = True inside plot and commenting this line out\n",
    "fig.set_yscale(\"log\")\n",
    "plt.grid(which = 'both', axis = 'both')\n",
    "plt.title('Petroleum produced by top 3 copmpanies per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Petroleum production in $m^3$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a49a0",
   "metadata": {},
   "source": [
    "# We are interested in observing oil production about depth to determine at what depth the productive formations are located and to assess the correlation between production and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depth = df_final.groupby(['profundidad'])['prod_pet'].sum().reset_index()\n",
    "\n",
    "# Tu código original con sns.lineplot modificado a sns.scatterplot\n",
    "fig = sns.scatterplot(data=df_depth,\n",
    "                      x='profundidad',\n",
    "                      y='prod_pet',\n",
    "                      palette='coolwarm_r',\n",
    "                      alpha=0.7,\n",
    "                      legend=False\n",
    "                      )\n",
    "\n",
    "plt.grid(True, which='both', axis='both')\n",
    "plt.show()\n",
    "\n",
    "# Suponiendo que df_depth es tu DataFrame\n",
    "correlation_matrix = df_depth[['profundidad', 'prod_pet']].corr()\n",
    "\n",
    "# Mostrar la matriz de correlación\n",
    "print(correlation_matrix)\n",
    "# Obtener el coeficiente de correlación específico entre 'profundidad' y 'prod_pet'\n",
    "correlation_value = correlation_matrix.loc['profundidad', 'prod_pet']\n",
    "print(f\"Coeficiente de correlación entre 'profundidad' y 'prod_pet': {correlation_value}\")\n",
    "\n",
    "\n",
    "#LA CORRELACION DE LA PRODUCCION DE PETROLEO CON LA PROFUNDIAD ES MODERADA A ALTA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
